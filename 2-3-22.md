# Log 2-3-22

## New this week

### Summary

* Added the option to train Tacotron 2 with speaker embeddings

### Speaker embeddings

[Implemented](https://github.com/mattm458/tacotron2/blob/speaker-embedding/model/tacotron2.py#L194) based on [this paper](https://proceedings.mlr.press/v80/skerry-ryan18a/skerry-ryan18a.pdf).

![Tacotron 2 with GST](img/speaker_embedding.png)

The goal is to allow training on large-scale TTS corpora such as [LibriTTS](https://openslr.org/60/), which contains a mixture of many speaking styles from many different speakers. The technique is used in the paper linked above as part of a prosody transfer mechanism for Tacotron, and is trained on both a single-speaker dataset (the 2013 Blizzard Challenge dataset with expressive speech) and a multi-speaker dataset (a proprietary dataset with different accents). Speaker embeddings are used in other expressive speech projects: for example, [Mellotron](https://ieeexplore.ieee.org/abstract/document/9054556) uses them in conjunction with several speech corpora including LJSpeech, a proprietary single-speaker dataset, and LibriTTS. The idea is that individual speaker characteristics can be disentangled from both the input text and prosodic characteristics of the output speech, leading to a model which is capable of a broad range of expression learned from many different speakers.

### Training procedure

The training procedure is a simple augmentation of the original Tacotron, and only requires a speaker ID to be supplied with the input data. No other alteration is necessary, and the Tacotron implementation linked above is capable of feeding the speaker ID into an embedding layer.

All prior considerations regarding learning proper attention alignment apply. Individual speakers should be monitored for alignment as training progresses, and it is occasionally desirable to train on speakers with large amounts of data available first before fine-tuning for speakers with little data.

### Future Work

Similar to the training situation described by Mellotron, we have access to large-scale general-purpose TTS datasets. Additionally, through B-MIC and other similar datasets, we have access to comparatively short recordings of conversations where a speaker is not reading from a text, but responding to another person who is speaking. It may be possible to train a TTS on large-scale general-purpose TTS data, then fine-tune it for use in conversations. Speaker embeddings will be an important component of this, as datasets like B-MIC contain limited amounts of speech for each unique speaker.