# Log 7-7-22

### Summary

This week, I arrived at what I believe is the most optimal model architecture for incorporating word embeddings. It outperforms everything else I've done before. I also have much more insight into the attention mechanism, and can produce charts showing transcripts with associated attention scores.

### Training with Word Embeddings, version 2

Last week, I tried the following architecture for incorporating word embeddings into the model:

![Word embedding diagram](img/6-30-22/words.png)

However, I observed that a dialogue engine likely has access to text from *the turn whose features are currently being predicted*, and this was not reflected in the model architecture.

Additionally, in this version of the model, the *dialogue-level speech feature embedding was used to produce a turn-level transcript embedding*, which I now believe is backward. My rationale is that, while word selection is important, the speech features contain much less information than the words. So it is probably better to allow the semantics of the transcript to dictate attention. 

The improved and hopefully somewhat streamlined model is shown below, with additional detail. It is not as different as it looks, only some parts of the diagram were restructured for clarity:

![Word embedding diagram](img/7-7-22/embeddings-v2.png)

The model contains two separate, unrelated encoding steps:

1. GloVe embeddings of the transcript are encoded into turn-level text embeddings. The text encoder has its own attention mechanism to identify important words which is isolated from the rest of the model. *The turn-level encoded text is treated as a feature, and is concatenated with the input speech features*.
2. Input features, including speech features, the turn speaker, and turn-level encoded text, are encoded as a sequence.

Separately from this, the textual transcript of the turn currently being predicted is encoded as a turn-level text embedding. This upcoming-turn text embedding is used in both the attention mechanism (deciding which input features are important) and in the decoder (as additional information to help it predict the output speech features).

| Performance | Training | Training loss | Evaluation | Evaluation loss | Teacher Forcing | L1 Loss    | Checkpoint | Extra Data                                                                                                                              |
|-------------|----------|---------------|------------|-----------------|-----------------|------------|------------|------------------------------------------------------------------------------------------------------------------------------------|
| Low        | Us       | Us            | Us         | Us              | Us              | 0.3815 | 229        |                                                                                            None                                        |                                                    |
| Medium        | Us       | Us            | Us         | Us              | Us              | 0.3663 | 306        |                                                                                 Word embeddings (old)                                                   |                                                    |
| High        | Us       | Us            | Us         | Us              | Us              | **0.3121** | 310     |                                                                                 Word embeddings  (V2)                                                  |                                                    |

\* Note - I had initially thought I reached a plateau, but the validation set is still improving. The numbers above are from a recent pause in training, but I have reason to believe the V2 model will continue to get better.

### Attention revisited

![Attention example 1](img/7-7-22/att-1.png)

In this example, it appears we are clarifying the call topic. The attention mechanism found the most recent turn the most relevant, followed by a prior turn where our partner discussed the call topic.

![Attention example 2](img/7-7-22/att-2.png)

In this example, our partner kicks off the main discussion at line 16. We start talking about going to the movies. The attention mechanism finds our turns where we discussed movies the most relevant (21, 19, and 17). However, it also found our partner's prompts at line 16 almost equally relevant, suggesting that their initial push to begin the discussion still has some influence on our current speaking style.

![Attention example 3](img/7-7-22/att-3.png)

In this example, we are responding to a prompt about hobbies. The attention mechanism found that, for this turn, nearly all of our partner's utterances are relevant, especially their introduction (2) and their most recent prompt to discuss our hobbies (8). Our prior responses are not very useful - understandable, since most are short inconsequential utterances (1, 3, 5). However, our own response about hobbies (7) was also not judged to be very relevant, so my guess is that this person is highly responsive to their partner.

![Attention example 3](img/7-7-22/att-4.png)

In this example, we are discussing the place where we live. The attention mechanism found that all prior turns from our partner specifically discussing location are the most relevant for this turn (4, 6, 8).


### What's next

I am continuing work on integrating with the dialogue engine. There are a few small things I want to tweak with the model, but unless there's anything else you want to try doing, I'm mostly happy with where it ended up.