# Log 3-10-22

### Summary

This week, I did two major things:

1. Worked on some additional evaluations for the prosody control conditioning I did last week.
2. Began an implementation of Wave RNN, which is ongoing.

### Tacotron prosody control results v3

One of my questions from last week was whether conditioning Tacotron output on a trained prosody control module actually helped, versus just continuing to train Tacotron for the 125 extra epochs. Here is the result of that test:

|   |  Vanilla Prosody Control |Vanilla Prosody Control (+125 epochs) | Tacotron with prosody conditioning  |
|---|--|-|---|
| Pitch Mean     | r=0.936 <br> ccc=0.922 <br>![](img/3-10-22/v-pitch-mean.png) | r=0.926 <br> ccc=0.917 <br>![](img/3-18-22/extra-pitch-mean.png) | r=0.941 <br> ccc=0.937 <br> ![](img/3-10-22/c-pitch-mean.png) |
| Pitch Range    | r=0.099 <br> ccc=0.076 <br> ![](img/3-10-22/v-pitch-range.png) | r=0.192 <br> ccc=0.149 <br>![](img/3-18-22/extra-pitch-range.png) | r=0.251 <br> ccc=0.185 <br> ![](img/3-10-22/c-pitch-range.png) |
| Intensity Mean | r=0.954 <br> ccc=0.567 <br> ![](img/3-10-22/v-intensity-mean.png)| r=0.954 <br> ccc=0.546 <br>![](img/3-18-22/extra-intensity-mean.png) | r=0.979 <br> ccc=0.918 <br> ![](img/3-10-22/c-intensity-mean.png) |
| Jitter         | r=0.712 <br> ccc=0.616 <br> ![](img/3-10-22/v-jitter.png)| r=0.653 <br> ccc=0.564 <br>![](img/3-18-22/extra-jitter.png) | r=0.727 <br> ccc=0.482 <br> ![](img/3-10-22/c-jitter.png) |
| Shimmer        | r=0.210 <br> ccc=0.014 <br> ![](img/3-10-22/v-shimmer.png)| r=0.311 <br> ccc=0.022 <br>![](img/3-18-22/extra-shimmer.png) | r=0.291 <br> ccc=0.022 <br>![](img/3-10-22/c-shimmer.png) |
| NHR            | r=0.558 <br> ccc=0.127 <br> ![](img/3-10-22/v-nhr.png)| r=0.546 <br> ccc=0.141 <br>![](img/3-18-22/extra-nhr.png) | r=0.635<br> ccc=0.313 <br> ![](img/3-10-22/c-nhr.png)  |
| Duration       | r=0.425 <br> ccc=0.309 <br> ![](img/3-10-22/v-duration.png)| r=0.342 <br> ccc=0.254 <br>![](img/3-18-22/extra-duration.png) | r=0.350<br> ccc=0.259 <br> ![](img/3-10-22/c-duration.png) |

Looking at a combination of these numbers and the [prosody control model results](https://github.com/mattm458/logs/blob/main/3-3-22.md), it appears that:

* The prosody model definitely makes a difference for features that the prosody model is good at detecting (mean pitch, mean intensity, and NHR).
* For features that the prosody model isn't as good at detecting (duration, shimmer, and pitch range), the results are mixed. Duration and pitch range got better, shimmer was worse.
* I can't explain jitter: the model is pretty good at detecting it, but the results are mixed here. The r-value is better, but the correlation coefficient is worse.

### Normalized prosody control results

I also tried again with a different normalization method applied to the features, and this is actually what is done in the original paper. I had originally neglected to do this because my initial impression was that restricting them to a range of (-1, 1) was only done for the benefit of a separate prediction model (the paper predicts appropriate prosodic features from an input text, whereas I don't care and just want to specify it manually). However, I wondered if this might improve the results of my prosody prediction model, so I tried it.

Prosody control model results:

| | Prosody predictor output | Prosody predictor output (V2 normalization)
|-|--------------------------|-|
|Pitch Mean     | r=0.905 <br> ![](img/3-3-22/pred-pitch-mean.png) | r=0.937 <br> ccc=0.912 <br> ![](img/3-18-22/prosodyv2-pitch-mean.png) |
| Pitch Range	| r=0.509 <br> ![](img/3-3-22/pred-pitch-range.png)| r=0.729 <br>ccc=0.703<br> ![](img/3-18-22/prosodyv2-pitch-range.png) |
| Intensity Mean | r=0.946 <br> ![](img/3-3-22/pred-intensity-mean.png)| r=0.962<br>ccc=0.956 <br> ![](img/3-18-22/prosodyv2-intensity-mean.png) |
| Jitter         | r=0.809 <br> ![](img/3-3-22/pred-jitter.png)| r=0.831<br>ccc=0.821 <br> ![](img/3-18-22/prosodyv2-jitter.png) |
| Shimmer        | r=0.567 <br> ![](img/3-3-22/pred-shimmer.png)| r=0.603<br>ccc=0.553 <br> ![](img/3-18-22/prosodyv2-shimmer.png) |
| NHR            | r=0.811 <br> ![](img/3-3-22/pred-nhr.png)| r=0.838<br>ccc=0.821 <br> ![](img/3-18-22/prosodyv2-nhr.png) |
| Duration       | r=0.754 <br> ![](img/3-3-22/pred-duration.png)| r=0.819<br>ccc=0.799 <br> ![](img/3-18-22/prosodyv2-duration.png) |

With these new numbers, the prosody predctor output is universally better for all features (especially pitch range!)

Results on tacotron output (without conditioning) are shown below:

|   |  Vanilla Prosody Control |Vanilla Prosody Control (V2 normalization) |
|---|--|-|
| Pitch Mean     | r=0.936 <br> ccc=0.922 <br>![](img/3-10-22/v-pitch-mean.png) | r=0.947 <br> ccc=0.921 <br>![](img/3-18-22/normalize-pitch-mean.png) |
| Pitch Range    | r=0.099 <br> ccc=0.076 <br> ![](img/3-10-22/v-pitch-range.png) | r=0.630 <br> ccc=0.379 <br>![](img/3-18-22/normalize-pitch-range.png) |
| Intensity Mean | r=0.954 <br> ccc=0.567 <br> ![](img/3-10-22/v-intensity-mean.png)| r=0.948 <br> ccc=0.509 <br>![](img/3-18-22/normalize-intensity-mean.png) |
| Jitter         | r=0.712 <br> ccc=0.616 <br> ![](img/3-10-22/v-jitter.png)| r=0.585 <br> ccc=0.456 <br>![](img/3-18-22/normalize-jitter.png) |
| Shimmer        | r=0.210 <br> ccc=0.014 <br> ![](img/3-10-22/v-shimmer.png)| r=0.232 <br> ccc=0.017 <br>![](img/3-18-22/normalize-shimmer.png) |
| NHR            | r=0.558 <br> ccc=0.127 <br> ![](img/3-10-22/v-nhr.png)| r=0.547 <br> ccc=0.151 <br>![](img/3-18-22/normalize-nhr.png) |
| Duration       | r=0.425 <br> ccc=0.309 <br> ![](img/3-10-22/v-duration.png)| r=0.466 <br> ccc=0.333 <br>![](img/3-18-22/normalize-duration.png) |

I believe the improvement in pitch range came from a slight tweak in how I was calculating it (I noticed the paper computes it based on the 95th and 5th percentiles, whereas I was just using Praat pitch output directly). I'm not sure how to interpret the rest of the results, though I think it might become clearer once I finish training a Tacotron model conditioned on this new prosody prediction model. This is in progress. I am expecting it to be much better due to the better quality of the prosody prediction model.

### Vocoder

I am currently working on an implementation of WaveRNN. As I mentioned in the last update, the official Tacotron paper uses Wavenet, but in practice different Tacotron implementations use any of a number of different neural vocoders (Waveglow, WaveRNN, Wavenet, and a few others) and it appears to be generally unclear if one is better than another.

This is ongoing and I am hoping to have it working by next week. I chose WaveRNN because I am hypothesizing (admittedly without much evidence) that it will do better being conditioned on prosodic features because of the recurrent aspect. However, I am concerned about maintaining prosody consistently across the entire output: most implementations of all vocoders break the input into small chunks instead of operating on the entire Mel spectrogram, and I'm concerned that later chunks may not have the benefit of the RNN's hidden layer. But we'll have to see.

### Questions and next steps

Here are my immediate next steps:

* Finish the WaveRNN implementation.
	- Test WaveRNN on prosodic conditioning, with an emphasis on jitter, shimmer, and NHR
* Finish training Tacotron conditioned on the new prosody predictor (this is currently training) and evaluate against the previous results.

After this, I'm not sure where to go - I'd like to take a step back and evaluate where we are and if it's worth trying to engineer a dialogue system as a secondary project. Also, I'm starting to do research into Tacotron in a conversational context after finding [this paper](https://arxiv.org/pdf/2106.06233.pdf): they condition Tacotron output on a context vector accumulated from conversation history. But we can talk about that next week.